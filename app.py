import io
import re
from typing import Optional, Tuple, List

import streamlit as st
import pandas as pd

# ---------- Helpers ----------
def clean_phone(s: str) -> Optional[str]:
    """–û—á–∏—â–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ +
    digits = re.sub(r"[^\d+]", "", s)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ +
    digits = re.sub(r'\++', '+', digits)
    
    # –£–±–∏—Ä–∞–µ–º + –Ω–µ –≤ –Ω–∞—á–∞–ª–µ
    if '+' in digits[1:]:
        digits = digits[0] + digits[1:].replace('+', '')
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã
    core = re.sub(r"\D", "", digits)
    
    # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –Ω–æ–º–µ—Ä–∞: 10 —Ü–∏—Ñ—Ä (0XXXXXXXXX) –∏–ª–∏ 12 —Ü–∏—Ñ—Ä (+380XXXXXXXXX)
    if len(core) == 10 and core.startswith('0'):
        return core
    elif len(core) == 12 and core.startswith('380'):
        return '+' + core
    elif len(core) == 9:  # –ë–µ–∑ –ø–µ—Ä–≤–æ–≥–æ 0
        return '0' + core
    elif 9 <= len(core) <= 13:  # –î—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
        return digits
    
    return None


def find_all_phones(text: str) -> List[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
    candidates = []
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    
    # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    patterns = [
        # +380XXXXXXXXX (—Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏)
        r'\+\s?380[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d',
        # 380XXXXXXXXX (–±–µ–∑ +)
        r'\b380[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d',
        # 0XXXXXXXXX (—É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç)
        r'\b0[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d',
        # –õ—é–±—ã–µ 10 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥ (—Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏)
        r'\b\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d',
        # –ü—Ä–æ—Å—Ç–æ 10 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥
        r'\b\d{10}\b',
        # 9 —Ü–∏—Ñ—Ä (–º–æ–∂–µ—Ç –Ω–µ —Ö–≤–∞—Ç–∞—Ç—å –ø–µ—Ä–≤–æ–≥–æ 0)
        r'\b\d{9}\b',
    ]
    
    for pattern in patterns:
        for match in re.finditer(pattern, text):
            phone_raw = match.group(0)
            phone = clean_phone(phone_raw)
            if phone and phone not in candidates:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –¥–∞—Ç–∞ –∏–ª–∏ ID
                digits_only = re.sub(r'\D', '', phone)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥–∞—Ç—É (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 19, 20, 01-31)
                if len(digits_only) == 8 and (digits_only.startswith('19') or digits_only.startswith('20')):
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤—Å–µ —Ü–∏—Ñ—Ä—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
                if len(set(digits_only)) == 1:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ)
                if len(digits_only) > 13:
                    continue
                
                candidates.append(phone)
    
    return candidates


def best_phone(text: str) -> Optional[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω"""
    phones = find_all_phones(text)
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ –Ω–æ–º–µ—Ä–∞ —Å +380 –∏–ª–∏ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å 0
    priority_phones = [p for p in phones if p.startswith('+380') or p.startswith('0')]
    if priority_phones:
        return priority_phones[0]
    
    return phones[0] if phones else None


def read_pdf(file_bytes: bytes) -> str:
    try:
        import pdfplumber
    except Exception:
        return ""
    text_parts = []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            if t:
                text_parts.append(t)
    return "\n".join(text_parts)


def read_docx(file_bytes: bytes) -> str:
    try:
        from docx import Document
    except Exception:
        return ""
    f = io.BytesIO(file_bytes)
    doc = Document(f)
    parts = []
    for p in doc.paragraphs:
        txt = p.text.strip()
        if txt:
            parts.append(txt)
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                t = cell.text.strip()
                if t:
                    parts.append(t)
    return "\n".join(parts)


def read_file(uploaded_file) -> str:
    data = uploaded_file.read()
    name = uploaded_file.name.lower()
    if name.endswith(".pdf"):
        return read_pdf(data)
    if name.endswith(".docx"):
        return read_docx(data)
    try:
        return data.decode("utf-8", errors="ignore")
    except Exception:
        return ""


# ---------- Extraction from text ----------
def extract_fio_from_text(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –§–ò–û –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ä–µ–∑—é–º–µ"""
    if not text:
        return None
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r'\s+', ' ', text)
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    
    # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 50 —Å—Ç—Ä–æ–∫–∞—Ö
    search_lines = lines[:50]
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –§–ò–û - –û–ß–ï–ù–¨ –≥–∏–±–∫–∏–µ
    fio_patterns = [
        # 3 —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π (–§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ)
        r'([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{2,})\s+([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{2,})\s+([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{2,})',
        # 2 —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π (–§–∞–º–∏–ª–∏—è –ò–º—è) - –º–∏–Ω–∏–º—É–º 2 –±—É–∫–≤—ã
        r'([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{1,})\s+([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{1,})',
    ]
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ - —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
    stop_words = [
        '—Ä–µ–∑—é–º–µ', 'curriculum', 'vitae', 
        'email', 'www', 'http', 'https',
        '—Ä–æ–∑–≥–ª—è–¥–∞—î', '—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç',
        '—è–Ω–≤–∞—Ä', '—Ñ–µ–≤—Ä–∞–ª', '–º–∞—Ä—Ç', '–∞–ø—Ä–µ–ª', '–º–∞–π', '–∏—é–Ω', '–∏—é–ª', '–∞–≤–≥—É—Å—Ç', '—Å–µ–Ω—Ç—è–±—Ä', '–æ–∫—Ç—è–±—Ä', '–Ω–æ—è–±—Ä', '–¥–µ–∫–∞–±—Ä',
        '—Å—ñ—á–Ω—è', '–ª—é—Ç–æ–≥–æ', '–±–µ—Ä–µ–∑–Ω—è', '–∫–≤—ñ—Ç–Ω—è', '—Ç—Ä–∞–≤–Ω—è', '—á–µ—Ä–≤–Ω—è', '–ª–∏–ø–Ω—è', '—Å–µ—Ä–ø–Ω—è', '–≤–µ—Ä–µ—Å–Ω—è', '–∂–æ–≤—Ç–Ω—è', '–ª–∏—Å—Ç–æ–ø–∞–¥–∞', '–≥—Ä—É–¥–Ω—è',
        'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',
        '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–≤—Ç–æ—Ä–Ω–∏–∫', '—Å—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä–≥', '–ø—è—Ç–Ω–∏—Ü–∞', '—Å—É–±–±–æ—Ç–∞', '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ',
        '–ø–æ–Ω–µ–¥—ñ–ª–æ–∫', '–≤—ñ–≤—Ç–æ—Ä–æ–∫', '—Å–µ—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä', "–ø'—è—Ç–Ω–∏—Ü—è", '—Å—É–±–æ—Ç–∞', '–Ω–µ–¥—ñ–ª—è',
        'university', '—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—ñ–Ω—Å—Ç–∏—Ç—É—Ç', 'institute', '–∞–∫–∞–¥–µ–º—ñ—è', 'academy', '—à–∫–æ–ª–∞', 'school',
        '–æ—Å–≤—ñ—Ç–∞', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', 'education', '–¥–æ—Å–≤—ñ–¥', '–æ–ø—ã—Ç', 'experience'
    ]
    
    # –°–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π - –ù–ï —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –§–ò–û –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä—è–¥–æ–º
    position_words = [
        '–º–µ–Ω–µ–¥–∂–µ—Ä', 'manager', '–∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä', 'administrator', '–ø—Ä–æ–¥–∞–≤–µ—Ü—å', '–ø—Ä–æ–¥–∞–≤–µ—Ü',
        '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', 'specialist', '–ø–æ–º—ñ—á–Ω–∏–∫', '–ø–æ–º–æ—â–Ω–∏–∫',
        '–æ–ø–µ—Ä–∞—Ç–æ—Ä', 'operator', '–¥–∏—Ä–µ–∫—Ç–æ—Ä', 'director', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '—Ä–µ–∫—Ä—É—Ç–µ—Ä', 'recruiter'
    ]
    
    candidates = []
    
    for idx, line in enumerate(search_lines):
        line_lower = line.lower()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        # Email –∏ URL
        if re.search(r'@|https?://|www\.', line):
            continue
        
        # –°—Ç—Ä–æ–∫–∏ —Å –¥–æ–º–µ–Ω–∞–º–∏
        if re.search(r'\.(com|ru|ua|org|net|gov)', line):
            continue
        
        # –î–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–¥.–º–º.–≥–≥–≥–≥ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –≥–æ–¥
        if re.search(r'\b\d{2}\.\d{2}\.\d{4}\b|\b(199|200|201|202)\d\b', line):
            continue
        
        # –°—Ç—Ä–æ–∫–∏ –≥–¥–µ –±–æ–ª—å—à–µ 10 —Ü–∏—Ñ—Ä (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–æ–º–µ—Ä–∞/–∫–æ–¥—ã)
        digit_count = sum(c.isdigit() for c in line)
        if digit_count > 10:
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤—Å—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        if any(line_lower.startswith(word) for word in stop_words):
            if not any(pw in line_lower for pw in position_words):
                continue
        
        # –ò—â–µ–º –§–ò–û –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern in fio_patterns:
            matches = list(re.finditer(pattern, line))
            for match in matches:
                fio = ' '.join(match.groups())
                words = fio.split()
                
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
                if len(words) < 2:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–ª–æ–≤–∞ –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                if any(word.lower() in stop_words for word in words):
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ - –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
                if all(word.lower() in position_words for word in words):
                    continue
                
                # –°—á–∏—Ç–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                priority = 100 - idx  # –ü–æ–∑–∏—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                
                # –ë–æ–Ω—É—Å—ã
                if len(words) == 3:  # –§–ò–û —Å –æ—Ç—á–µ—Å—Ç–≤–æ–º
                    priority += 60
                elif len(words) == 2:
                    # –°–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–ª–∏–Ω—É —Å–ª–æ–≤
                    avg_len = sum(len(w) for w in words) / len(words)
                    if avg_len >= 5:  # –ü–æ–ª–Ω—ã–µ —Å–ª–æ–≤–∞
                        priority += 40
                    elif avg_len >= 3:  # –°—Ä–µ–¥–Ω–∏–µ/—Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ
                        priority += 25
                    else:
                        priority += 10
                
                # –û–≥—Ä–æ–º–Ω—ã–π –±–æ–Ω—É—Å –µ—Å–ª–∏ –≤ –ø–µ—Ä–≤—ã—Ö 3 —Å—Ç—Ä–æ–∫–∞—Ö
                if idx < 3:
                    priority += 100
                elif idx < 10:
                    priority += 50
                
                # –ë–æ–Ω—É—Å –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–æ—Ä–æ—Ç–∫–∞—è (–≤–µ—Ä–æ—è—Ç–Ω–æ —Ç–æ–ª—å–∫–æ –§–ò–û)
                if len(line) < 60:
                    priority += 30
                
                # –®—Ç—Ä–∞—Ñ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ–ª–∂–Ω–æ—Å—Ç—å –≤ —Å—Ç—Ä–æ–∫–µ
                if any(pw in line_lower for pw in position_words):
                    priority -= 10
                
                candidates.append((priority, fio, idx, line))
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    if candidates:
        candidates.sort(key=lambda x: x[0], reverse=True)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        best = candidates[0]
        return best[1]
    
    return None


def extract_position_from_text(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∂–µ–ª–∞–µ–º—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å –∏–∑ —Ä–µ–∑—é–º–µ"""
    if not text:
        return None
    
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
    position_patterns = [
        r'(?i)(?:–∂–µ–ª–∞–µ–º–∞—è|–±–∞–∂–∞–Ω–∞)\s+(?:–¥–æ–ª–∂–Ω–æ—Å—Ç—å|–ø–æ—Å–∞–¥–∞)[:\s\-‚Äî]*(.+?)(?:\n|$)',
        r'(?i)(?:–¥–æ–ª–∂–Ω–æ—Å—Ç—å|–ø–æ—Å–∞–¥–∞)[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)(?:–≤–∞–∫–∞–Ω—Å–∏—è|–≤–∞–∫–∞–Ω—Å—ñ—è)[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)(?:—Ä–æ–∑–≥–ª—è–¥–∞—î|—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç)\s+(?:–ø–æ—Å–∞–¥–∏|–¥–æ–ª–∂–Ω–æ—Å—Ç–∏)[:\s\-‚Äî]*(.+?)(?:\n|$)',
        r'(?i)(?:–ø–æ–∑–∏—Ü–∏—è|–ø–æ–∑–∏—Ü—ñ—è)[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)position[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)objective[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)(?:—Ü–µ–ª—å|—Ü—ñ–ª—å)[:\s\-‚Äî]+(.+?)(?:\n|$)',
        r'(?i)(?:–∏—â—É|—à—É–∫–∞—é)\s+(?:—Ä–∞–±–æ—Ç—É|—Ä–æ–±–æ—Ç—É)[:\s\-‚Äî]*(.+?)(?:\n|$)',
    ]
    
    # –ò—â–µ–º –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É
    for pattern in position_patterns:
        match = re.search(pattern, text[:4000])
        if match:
            position = match.group(1).strip()
            # –û—á–∏—Å—Ç–∫–∞
            position = re.sub(r'[_\-‚Äî\.]+$', '', position).strip()
            position = position.split('\n')[0].strip()
            
            # –£–±–∏—Ä–∞–µ–º ID –∏ –Ω–æ–º–µ—Ä–∞ –≤ –∫–æ–Ω—Ü–µ
            position = re.sub(r'\s+\d{6,}$', '', position)
            
            if 3 <= len(position) <= 200:
                return position
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ, –∏—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 40 —Å—Ç—Ä–æ–∫–∞—Ö
    position_keywords_uk = [
        '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä', '–ø—Ä–æ–¥–∞–≤–µ—Ü—å', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç',
        '–ø–æ–º—ñ—á–Ω–∏–∫', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '–∞—Å–∏—Å—Ç–µ–Ω—Ç', '–∫–µ—Ä—ñ–≤–Ω–∏–∫',
        '–±–∞—Ä–∏—Å—Ç–∞', '–æ—Ñ—ñ—Å-–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–µ–∫—Ä–µ—Ç–∞—Ä', '—Ä–µ–∫—Ä—É—Ç–µ—Ä'
    ]
    
    position_keywords_ru = [
        '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä', '–ø—Ä–æ–¥–∞–≤–µ—Ü', '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç',
        '–ø–æ–º–æ—â–Ω–∏–∫', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä', '–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å',
        '–±–∞—Ä–∏—Å—Ç–∞', '–æ—Ñ–∏—Å-–º–µ–Ω–µ–¥–∂–µ—Ä', '—Å–µ–∫—Ä–µ—Ç–∞—Ä—å', '—Ä–µ–∫—Ä—É—Ç–µ—Ä'
    ]
    
    position_keywords_en = [
        'manager', 'administrator', 'seller', 'consultant', 'specialist',
        'assistant', 'operator', 'coordinator', 'director', 'supervisor',
        'barista', 'recruiter', 'designer', 'developer', 'engineer'
    ]
    
    all_keywords = position_keywords_uk + position_keywords_ru + position_keywords_en
    
    for i, line in enumerate(lines[:40]):
        line_lower = line.lower()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
        if len(line) > 150:
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å email, url, –¥–∞—Ç–∞–º–∏
        if re.search(r'(@|https?://|www\.|\d{4}|\.com|\.ua|\.ru)', line):
            continue
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        if any(keyword in line_lower for keyword in all_keywords):
            # –£–±–∏—Ä–∞–µ–º –§–ò–û –∏–∑ —Å—Ç—Ä–æ–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            cleaned = line
            
            # –£–±–∏—Ä–∞–µ–º ID –Ω–æ–º–µ—Ä–∞
            cleaned = re.sub(r'\s+\d{6,}$', '', cleaned)
            
            # –ï—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –µ—Å—Ç—å —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∏ –¥–æ–ª–∂–Ω–æ—Å—Ç—å, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç—å
            words = cleaned.split()
            position_words = []
            
            for word in words:
                word_lower = word.lower()
                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å
                if any(kw in word_lower for kw in all_keywords):
                    position_words.append(word)
                # –ò–ª–∏ –µ—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –∏ —Ç–µ–∫—É—â–µ–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
                elif position_words and not re.match(r'^[–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+$', word):
                    position_words.append(word)
                elif position_words:
                    position_words.append(word)
            
            if position_words:
                position = ' '.join(position_words).strip()
                position = re.sub(r'[,;:.]+$', '', position)
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
                if 5 <= len(position) <= 150:
                    return position
            
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –±–µ—Ä–µ–º –≤—Å—é —Å—Ç—Ä–æ–∫—É
            if 5 <= len(cleaned) <= 100:
                return cleaned
    
    return None


def extract_position_from_filename(filename: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç—å –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    name_clean = re.sub(r"\.[^.]+$", "", filename)
    
    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º "‚Äî"
    if re.match(r'^\d+$', name_clean):
        return "‚Äî"
    
    try:
        from urllib.parse import unquote
        name_clean = unquote(name_clean)
    except:
        pass
    
    # –û—á–∏—Å—Ç–∫–∞
    for pattern in [r'(?i)workua', r'(?i)work\.ua', r'(?i)—Ä–µ–∑—é–º–µ', r'(?i)resume', r'(?i)\bcv\b']:
        name_clean = re.sub(pattern, ' ', name_clean)
    
    name_clean = re.sub(r'\b\d{10,}\b', ' ', name_clean)
    name_clean = re.sub(r'\d{2,4}[-./]\d{1,2}[-./]\d{2,4}', ' ', name_clean)
    name_clean = re.sub(r"[_\-‚Äî,\.]+", " ", name_clean)
    name_clean = re.sub(r'\s+', ' ', name_clean).strip()
    
    return name_clean if name_clean else "‚Äî"


# ---------- UI ----------
st.set_page_config(page_title="CV Extractor", page_icon="üìÑ", layout="centered")
st.title("üìÑ –ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–∞–Ω–∏—Ö –∑ —Ä–µ–∑—é–º–µ")
st.caption("–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ü–Ü–ë, –ø–æ—Å–∞–¥–∏ —Ç–∞ —Ç–µ–ª–µ—Ñ–æ–Ω—É")

debug_mode = st.sidebar.checkbox("üîç –†–µ–∂–∏–º –≤—ñ–¥–ª–∞–¥–∫–∏", value=False)
show_stats = st.sidebar.checkbox("üìä –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É", value=True)

uploaded = st.file_uploader(
    "–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª–∏ —Ä–µ–∑—é–º–µ (PDF/DOCX)", 
    accept_multiple_files=True,
    type=["pdf", "docx"],
)

if uploaded:
    rows = []
    debug_info = []
    stats = {'fio_found': 0, 'position_found': 0, 'phone_found': 0, 'total': len(uploaded)}
    
    with st.spinner(f'–û–±—Ä–æ–±–∫–∞ {len(uploaded)} —Ñ–∞–π–ª—ñ–≤...'):
        for uf in uploaded:
            text = read_file(uf)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            fio = extract_fio_from_text(text) or "‚Äî"
            position_text = extract_position_from_text(text)
            position_filename = extract_position_from_filename(uf.name)
            position = position_text if position_text else position_filename
            phone = best_phone(text) or "‚Äî"
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if fio != "‚Äî":
                stats['fio_found'] += 1
            if position != "‚Äî":
                stats['position_found'] += 1
            if phone != "‚Äî":
                stats['phone_found'] += 1
            
            rows.append({
                "–§–∞–π–ª": uf.name,
                "–ü–Ü–ë": fio,
                "–ë–∞–∂–∞–Ω–∞ –ø–æ—Å–∞–¥–∞": position,
                "–¢–µ–ª–µ—Ñ–æ–Ω": phone,
            })
            
            if debug_mode:
                all_phones = find_all_phones(text)
                
                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –§–ò–û –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                fio_candidates = []
                lines_preview = [l.strip() for l in text.split('\n') if l.strip()][:50]
                
                for idx, line in enumerate(lines_preview):
                    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –≤—Å–µ—Ö —Å–ª–æ–≤ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                    pattern = r'([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{1,})\s+([–ê-–Ø–Å–Ü–á–Ñ“êA-Z][–∞-—è—ë—ñ—ó—î“ëa-z]{1,})'
                    matches = re.finditer(pattern, line)
                    for match in matches:
                        candidate = ' '.join(match.groups())
                        fio_candidates.append(f"–°—Ç—Ä–æ–∫–∞ {idx+1}: {candidate} | –í—Å—è —Å—Ç—Ä–æ–∫–∞: {line[:80]}")
                
                # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ü–∏—Ñ—Ä
                digit_sequences = []
                # –ò—â–µ–º –ª—é–±—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 9-12 —Ü–∏—Ñ—Ä
                for match in re.finditer(r'\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d[\s\-\(\)\.]*\d', text[:2000]):
                    seq = match.group(0)
                    digit_sequences.append(seq)
                
                debug_info.append({
                    "–§–∞–π–ª": uf.name,
                    "–ü–Ü–ë (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)": fio,
                    "–ü–æ—Å–∞–¥–∞ (–∑ —Ç–µ–∫—Å—Ç—É)": position_text or "‚Äî",
                    "–ü–æ—Å–∞–¥–∞ (–∑ –Ω–∞–∑–≤–∏)": position_filename,
                    "–í—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏": ', '.join(all_phones) if all_phones else "‚Äî",
                    "–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ –Ω–∞ –ü–Ü–ë": '\n'.join(fio_candidates[:15]) if fio_candidates else "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ",
                    "–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Ü–∏—Ñ—Ä": ', '.join(digit_sequences[:10]) if digit_sequences else "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ",
                    "–¢–µ–∫—Å—Ç (–ø–æ—á–∞—Ç–æ–∫)": text[:800] if text else "‚ùå –¢–µ–∫—Å—Ç –Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–æ"
                })
    
    df = pd.DataFrame(rows)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if show_stats:
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("üìÅ –í—Å—å–æ–≥–æ —Ñ–∞–π–ª—ñ–≤", stats['total'])
        col2.metric("üë§ –ü–Ü–ë –∑–Ω–∞–π–¥–µ–Ω–æ", f"{stats['fio_found']}/{stats['total']}")
        col3.metric("üíº –ü–æ—Å–∞–¥ –∑–Ω–∞–π–¥–µ–Ω–æ", f"{stats['position_found']}/{stats['total']}")
        col4.metric("üìû –¢–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ", f"{stats['phone_found']}/{stats['total']}")
    
    st.dataframe(df, use_container_width=True, height=400)
    
    # –û—Ç–ª–∞–¥–∫–∞
    if debug_mode and debug_info:
        st.subheader("üîç –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è")
        st.caption("–¢—É—Ç –ø–æ–∫–∞–∑–∞–Ω–æ —â–æ —Å–∞–º–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ–∂–Ω–æ–º—É —Ñ–∞–π–ª—ñ —Ç–∞ –í–°–Ü –º–æ–∂–ª–∏–≤—ñ –∫–∞–Ω–¥–∏–¥–∞—Ç–∏ –Ω–∞ –ü–Ü–ë")
        
        for info in debug_info:
            status = "‚úÖ" if info['–ü–Ü–ë (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)'] != "‚Äî" else "‚ùå"
            with st.expander(f"{status} {info['–§–∞–π–ª'][:60]}"):
                
                st.write("### üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç")
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**–ü–Ü–ë (–æ–±—Ä–∞–Ω–æ):**")
                    if info['–ü–Ü–ë (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)'] != "‚Äî":
                        st.success(info['–ü–Ü–ë (—Ä–µ–∑—É–ª—å—Ç–∞—Ç)'])
                    else:
                        st.error("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                    
                    st.write("**–¢–µ–ª–µ—Ñ–æ–Ω (–æ–±—Ä–∞–Ω–æ):**")
                    if info['–í—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏'] != "‚Äî":
                        st.success(info['–í—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏'].split(',')[0])
                    else:
                        st.error("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
                with col2:
                    st.write("**–ü–æ—Å–∞–¥–∞ –∑ —Ç–µ–∫—Å—Ç—É:**")
                    if info['–ü–æ—Å–∞–¥–∞ (–∑ —Ç–µ–∫—Å—Ç—É)'] != "‚Äî":
                        st.info(info['–ü–æ—Å–∞–¥–∞ (–∑ —Ç–µ–∫—Å—Ç—É)'])
                    else:
                        st.warning("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                    
                    st.write("**–í—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏:**")
                    if info['–í—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏'] != "‚Äî":
                        st.info(info['–í—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏'])
                    else:
                        st.error("–ñ–æ–¥–Ω–æ–≥–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
                st.write("### üë• –í—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ –∫–∞–Ω–¥–∏–¥–∞—Ç–∏ –Ω–∞ –ü–Ü–ë")
                st.text(info['–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ –Ω–∞ –ü–Ü–ë'])
                
                st.write("### üìû –ê–Ω–∞–ª—ñ–∑ –ø–æ—à—É–∫—É —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤")
                st.text(f"–ó–Ω–∞–π–¥–µ–Ω—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Ü–∏—Ñ—Ä: {info['–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Ü–∏—Ñ—Ä']}")
                
                st.write("### üìÑ –ü–æ—á–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–µ—Ä—à—ñ 800 —Å–∏–º–≤–æ–ª—ñ–≤)")
                st.code(info['–¢–µ–∫—Å—Ç (–ø–æ—á–∞—Ç–æ–∫)'], language='text')
    
    # CSV —ç–∫—Å–ø–æ—Ä—Ç
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV",
        data=csv,
        file_name="cv_extract.csv",
        mime="text/csv",
    )
else:
    st.info("üëÜ –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª–∏ —Ä–µ–∑—é–º–µ –¥–ª—è –æ–±—Ä–æ–±–∫–∏")

st.markdown("""
---
### üìã –Ø–∫ –ø—Ä–∞—Ü—é—î:
- **–ü–Ü–ë**: —à—É–∫–∞—î—Ç—å—Å—è –≤ –ø–µ—Ä—à–∏—Ö 35 —Ä—è–¥–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø—ñ–¥—Ç—Ä–∏–º—É—î –ø–æ–≤–Ω—ñ —Ç–∞ —Å–∫–æ—Ä–æ—á–µ–Ω—ñ —ñ–º–µ–Ω–∞
- **–ü–æ—Å–∞–¥–∞**: —à—É–∫–∞—î –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ "–†–æ–∑–≥–ª—è–¥–∞—î –ø–æ—Å–∞–¥–∏:", –∞–±–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î
- **–¢–µ–ª–µ—Ñ–æ–Ω**: –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–æ–º–µ—Ä–∏ (0XXXXXXXXX –∞–±–æ +380XXXXXXXXX)

### üí° –ü–æ—Ä–∞–¥–∏:
- –£–≤—ñ–º–∫–Ω—ñ—Ç—å "–†–µ–∂–∏–º –≤—ñ–¥–ª–∞–¥–∫–∏" —â–æ–± –ø–æ–±–∞—á–∏—Ç–∏, —â–æ —Å–∞–º–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ–∂–Ω–æ–º—É —Ñ–∞–π–ª—ñ
- –Ø–∫—â–æ –ü–Ü–ë –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ —â–æ –≤–æ–Ω–æ —î –≤ –ø–µ—Ä—à–∏—Ö —Ä—è–¥–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
""")